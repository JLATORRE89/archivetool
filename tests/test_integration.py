# tests/test_integration.py
import unittest
import os
import shutil
import tempfile
import time
from bs4 import BeautifulSoup

from archiver.core import WebsiteArchiver
from tests.test_server import TestServer


class TestArchiverIntegration(unittest.TestCase):
    """
    Integration tests for the Website Archiver using a real test server.
    These tests verify the archiver can properly crawl a site and capture dynamic content.
    """

    @classmethod
    def setUpClass(cls):
        """Set up the test server once for all tests"""
        cls.test_server = TestServer()
        cls.base_url = cls.test_server.setup()
        print(f"Test server running at {cls.base_url}")

    @classmethod
    def tearDownClass(cls):
        """Shut down the test server after all tests"""
        cls.test_server.shutdown()

    def setUp(self):
        """Set up before each test"""
        # Create a temporary output directory for each test
        self.output_dir = tempfile.mkdtemp()

    def tearDown(self):
        """Clean up after each test"""
        # Remove the output directory
        shutil.rmtree(self.output_dir)

    def test_basic_archive_functionality(self):
        """Test that the archiver can download a basic site"""
        # Create and run the archiver with the test server URL
        archiver = WebsiteArchiver(self.base_url, self.output_dir, wait_for_ajax=False)
        archiver.start_archive()
        
        # Check that the output files exist
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "index.html")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "style.css")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "script.js")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "page1.html")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "page2.html")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "subdir", "page3.html")))
        
        # Check for image files
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "images", "test1.png")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "images", "test2.png")))
        
        # Verify the log directory was created
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "logs")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "logs", "archiver.log")))

    def test_relative_urls_resolved(self):
        """Test that relative URLs are properly resolved"""
        # Create and run the archiver
        archiver = WebsiteArchiver(self.base_url, self.output_dir, wait_for_ajax=False)
        archiver.start_archive()
        
        # Load the archived page3 (which has relative URLs)
        with open(os.path.join(self.output_dir, "subdir", "page3.html"), 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Parse the content
        soup = BeautifulSoup(content, 'html.parser')
        
        # Check that the relative URL to the home page was resolved
        home_link = soup.find('a', text='Back to Home')
        self.assertIsNotNone(home_link)
        
        # The href should now be a path that works in the archived site
        self.assertTrue(home_link['href'].endswith('index.html'))
        
        # Verify that the CSS link was also properly resolved
        css_link = soup.find('link', rel='stylesheet')
        self.assertIsNotNone(css_link)
        
        # Check that we can access the linked pages from page3
        self.assertTrue(os.path.exists(os.path.join(
            self.output_dir, 
            os.path.normpath(os.path.join("subdir", home_link['href']))
        )))

    def test_dynamic_content_capture(self):
        """Test that dynamic content is captured with wait_for_ajax=True"""
        try:
            # Create and run the archiver with wait_for_ajax=True
            archiver = WebsiteArchiver(self.base_url, self.output_dir, wait_for_ajax=True)
            
            # If Selenium isn't available, this test should be skipped
            if not archiver.driver:
                self.skipTest("Selenium webdriver not available")
                
            archiver.start_archive()
            
            # Load the archived index.html
            with open(os.path.join(self.output_dir, "index.html"), 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse the content
            soup = BeautifulSoup(content, 'html.parser')
            
            # Check for dynamic content
            dynamic_content = soup.find(id="dynamic-content")
            self.assertIsNotNone(dynamic_content)
            
            # Look for specific text that should have been generated by JavaScript
            self.assertIn("Dynamically Generated Content", dynamic_content.text)
            
            # Check that loading text was updated
            loading_element = soup.find(id="loading")
            self.assertIsNotNone(loading_element)
            self.assertEqual("Content loaded!", loading_element.text)
        except Exception as e:
            print(f"Skipping dynamic content test due to error: {e}")
            self.skipTest(f"Dynamic content test failed: {e}")
        
    def test_image_processing(self):
        """Test that images are processed correctly"""
        # Create and run the archiver with image compression enabled
        archiver = WebsiteArchiver(
            self.base_url, 
            self.output_dir, 
            wait_for_ajax=False,
            compress_images=True,
            max_image_size_kb=1  # Set a small size to force compression
        )
        archiver.start_archive()
        
        # Check that the images were downloaded
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "images", "test1.png")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "images", "test2.png")))
        
        # In embedded mode, the images should be base64 encoded in the HTML
        with open(os.path.join(self.output_dir, "index.html"), 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Check if the image is referenced as a data URI
        self.assertIn("data:", content)

    def test_multi_threading(self):
        """Test that multi-threading works correctly"""
        # Create and run the archiver with multiple threads
        archiver = WebsiteArchiver(
            self.base_url, 
            self.output_dir,
            max_threads=3  # Use multiple threads
        )
        
        # Record start time
        start_time = time.time()
        
        # Start the archive
        archiver.start_archive()
        
        # Record end time
        end_time = time.time()
        
        # Check that all pages were archived
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "index.html")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "page1.html")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "page2.html")))
        self.assertTrue(os.path.exists(os.path.join(self.output_dir, "subdir", "page3.html")))
        
        # Record execution time (for debugging, not an actual assertion)
        execution_time = end_time - start_time
        print(f"Multi-threading test completed in {execution_time:.2f} seconds")


if __name__ == '__main__':
    unittest.main()